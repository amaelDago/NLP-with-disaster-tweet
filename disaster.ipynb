{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599757569935",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n\n   target  \n0       1  \n1       1  \n"
    }
   ],
   "source": [
    "#Create process function and count occurs function\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "## Process Text\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv('train.csv')\n",
    "print(data.head(2))\n",
    "\n",
    "# Process function \n",
    "def process_tweet(tweet) : \n",
    "    # This function take a tweet and process it.\n",
    "    # input : a tweet\n",
    "    # output : tweet clean : a list of words containing a processed tweet \n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # English stopwords\n",
    "    en_stopword = stopwords.words(\"english\")\n",
    "\n",
    "    # Remove hashtag \n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # Remove \"$\"\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweet_clean = [stemmer.stem(word) for word in tweet_tokens if (word not in en_stopword and word not in string.punctuation)]\n",
    "\n",
    "    return tweet_clean\n",
    "\n",
    "\n",
    "# Definition of \n",
    "def build_freqs(tweets, yslist):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    #yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(str(tweet)):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "12929\n"
    }
   ],
   "source": [
    "# Get tweet and disaster real\n",
    "\n",
    "all_tweets = list(data.text)\n",
    "yslist = list(data.target)\n",
    "\n",
    "# Split data in train and test set \n",
    "cut = int(len(all_tweets)*3/4)\n",
    "train_x = all_tweets[:cut]\n",
    "train_y = yslist[:cut]\n",
    "\n",
    "test_x = all_tweets[cut:]\n",
    "test_y = yslist[cut :]\n",
    "\n",
    "# Get a freqs of words with feature \"text\"\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "print(len(freqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling\n",
    "\n",
    "# Define sigmoid function\n",
    "def sigmoid(z) : \n",
    "    # input : float of array/list of float\n",
    "    # output : array \n",
    "\n",
    "    if type(z) != np.ndarray : \n",
    "        z = np.array(z)\n",
    "    return 1/(1+ np.exp(-z))\n",
    "\n",
    "# Gradient descent function\n",
    "def gradientDescent(X, Y, learning_rate = 0.01, iteration = 100) : \n",
    "    \"\"\" Input:\n",
    "    X: matrix of features \n",
    "    Y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "    theta: weight vector of dimension (n+1,1)\n",
    "    alpha: learning rate\n",
    "    num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "    J: the final cost\n",
    "    theta: your final weight vector\n",
    "    \"\"\"\n",
    "    if type(X) != np.ndarray : \n",
    "        X = np.array(X)\n",
    "\n",
    "    if type(Y) != np.ndarray : \n",
    "        Y = np.array(Y)\n",
    "\n",
    "    # Number of rows\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Theta\n",
    "    #np.random.seed(1)\n",
    "    #theta = np.random.rand(X.shape[1]).reshape(-1,1)\n",
    "    theta = np.zeros((3,1))\n",
    "\n",
    "    # Loop over iteration\n",
    "    for i in range(iteration) : \n",
    "        #if i%70 : \n",
    "        #    print (J, theta)\n",
    "\n",
    "        # Compute of prediction\n",
    "        Z = np.dot(X, theta)\n",
    "\n",
    "        pred = sigmoid(Z)\n",
    "\n",
    "        # Compute cost function \n",
    "        J = -1/m * (np.dot(Y.T , np.log(pred)) + np.dot((1-Y).T , np.log(1 - pred)))\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (learning_rate/m) * np.dot(X.T, (pred - Y))\n",
    "\n",
    "        J = float(J)\n",
    "\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  1.  39. 113.]]\n"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "def extract_features(tweet, freqs) : \n",
    "    \"\"\"\n",
    "    iput : a tweet and a freqs dictionary of tuple\n",
    "    output : a feature vector of dimension 3\n",
    "    \"\"\"\n",
    "    list_of_word = process_tweet(tweet)\n",
    "    x = np.ones(3).reshape(1,-1)\n",
    "\n",
    "    # Loop on list of word from cleaned tweet\n",
    "    for word in list_of_word :\n",
    "        \n",
    "        x[0,1]=  freqs[(word, 1)] if (word, 1) in freqs else 0\n",
    "    \n",
    "        x[0,2] +=  freqs[(word, 0)] if (word, 0) in freqs  else 0\n",
    "\n",
    "    return x\n",
    "    \n",
    "# Test of function feature_extract\n",
    "print(extract_features(train_x[0], freqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5709, 3)\n(5709, 3)\n"
    }
   ],
   "source": [
    "# Training model\n",
    "\n",
    "# Extract feature for text\n",
    "X = np.zeros((len(train_x), 3))\n",
    "print(X.shape)\n",
    "\n",
    "for i, tweet in enumerate(train_x) : \n",
    "    X[i,:] = extract_features(tweet, freqs)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning hyper parameter\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class logisticRegression(BaseEstimator, RegressorMixin) :\n",
    "\n",
    "    def __init__(self, learning_rate = 10**(-8), iteration = 1600) : \n",
    "\n",
    "        #self.X = X\n",
    "        #self.Y = Y\n",
    "        #self.theta = theta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = iteration\n",
    "        \n",
    "\n",
    "    def fit(self, X, Y ) :\n",
    "\n",
    "        # Check if train_x and train_y are np.ndarray\n",
    "        #X = np.ones((len(X),3))\n",
    "\n",
    "        #for i, tweet in enumerate(X) : \n",
    "        #    X[i,:] = extract_features(tweet, freqs)\n",
    "\n",
    "        Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "        assert( type(X) == np.ndarray)\n",
    "        assert( type(X) == np.ndarray)\n",
    "\n",
    "        self.J, self.theta = gradientDescent(X, Y, learning_rate = self.learning_rate, iteration = self.iteration)  \n",
    "        return self\n",
    "\n",
    "    def predict(self, X) : \n",
    "        \"\"\"\n",
    "        input : tweet : le tweet\n",
    "        freqs : dictionary of tuple (word, label) and his frequency\n",
    "        theta : a weight vector\n",
    "        output  \n",
    "        prob : probability a tweet concern a real disaster or not\n",
    "        \"\"\" \n",
    "        #X = extract_features(tweet, freqs)\n",
    "\n",
    "        prob = float(sigmoid(np.dot(X, self.theta)))\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def score(self, X, Y) : \n",
    "        \"\"\"\n",
    "        input : \n",
    "        X : a list of tweet\n",
    "        Y : label of tweet\n",
    "        freqs : dictionary of pair (word, label) and his frequency\n",
    "        theta_hat : weight vector\n",
    "        output :\n",
    "        accuracy \n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "\n",
    "        m = len(X)\n",
    "\n",
    "        assert(len(X) == len(Y))\n",
    "\n",
    "        for i, x in enumerate(X) :\n",
    "\n",
    "            #feature = extract_features(tweet, freqs)\n",
    "\n",
    "            prob = float(sigmoid(np.dot(x, self.theta)))\n",
    "\n",
    "            pred_label = 1 if prob >= 0.5 else 0\n",
    "\n",
    "            if Y[i] == pred_label : \n",
    "                num_correct += 1\n",
    "\n",
    "        score = num_correct / m\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-1.09244489e-06]\n [ 1.13821565e-04]\n [-2.92350928e-04]] 0.6869191645540338\n"
    }
   ],
   "source": [
    "# Model fitting\n",
    "\n",
    "regLog = logisticRegression(learning_rate=10**-8)\n",
    "reglog = regLog.fit(X, np.array(train_y))\n",
    "\n",
    "# Printing of estimator theta and loss function J\n",
    "print(reglog.theta, reglog.J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The accuracy of model is 0.55\n"
    }
   ],
   "source": [
    "# Test of regression model\n",
    "\n",
    "# Modeling\n",
    "#Y = np.array(train_y).reshape(-1,1)\n",
    "\n",
    "#J, theta_hat = gradientDescent(X, Y, learning_rate = 0.000001, iteration = 400)\n",
    "#print(theta_hat, J)\n",
    "\n",
    "# Prediction\n",
    "def predict_tweet(tweet, freqs, theta) : \n",
    "    \"\"\"\n",
    "    input : tweet : le tweet\n",
    "    freqs : dictionary of pair (word, label) and his frequency\n",
    "    theta : a weight vector\n",
    "    output  \n",
    "    prob : probability a tweet concern a real disaster or not\n",
    "    \"\"\" \n",
    "    feature = extract_features(tweet, freqs)\n",
    "\n",
    "    prob = float(sigmoid(np.dot(feature, theta)))\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "    # Test of Regression logistique\n",
    "def test_regression_logistique(test_x, test_y, freqs, theta) : \n",
    "\n",
    "    \"\"\"\n",
    "    input : \n",
    "    X : a list of tweet\n",
    "    Y : label of tweet\n",
    "    freqs : dictionary of pair (word, label) and his frequency\n",
    "    theta_hat : weight vector\n",
    "    output :\n",
    "    accuracy \n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "\n",
    "    m = len(test_x) \n",
    "\n",
    "    assert(len(test_x) == len(test_y))\n",
    "\n",
    "    for i, tweet in enumerate(test_x) : \n",
    "        prob = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        pred_label = round(prob)\n",
    "\n",
    "        if test_y[i] == pred_label : \n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / m\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Accuracy\n",
    "accuracy = test_regression_logistique(test_x, test_y, freqs, reglog.theta)\n",
    "\n",
    "print(\"The accuracy of model is {:4.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(7613, 3)\nScores from cross validation 7612 are : 0.6316480630334865.\nScores from cross validation 7612 are : 0.5883125410374261.\nScores from cross validation 7612 are : 0.525279054497702.\nScores from cross validation 7612 are : 0.5919842312746386.\nScores from cross validation 7612 are : 0.533508541392904.\nModel accuracy is : 0.5741464862472314\n"
    }
   ],
   "source": [
    "# Cross validation function\n",
    "\n",
    "# Extract feature for text\n",
    "XX = np.zeros((len(all_tweets), 3))\n",
    "print(XX.shape)\n",
    "\n",
    "for i, tweet in enumerate(all_tweets) : \n",
    "    XX[i,:] = extract_features(tweet, freqs)\n",
    "\n",
    "\n",
    "scores = cross_val_score(regLog, XX, np.array(yslist), cv=5)\n",
    "for score in scores :  \n",
    "    print(\"Scores from cross validation {} are : {}.\".format(i,score))\n",
    "\n",
    "print('Model accuracy is : {}'.format(np.mean(scores)))\n"
   ]
  }
 ]
}