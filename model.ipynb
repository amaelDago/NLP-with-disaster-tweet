{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnKjHo5gkuup"
      },
      "source": [
        "# Disaster Tweet : True or False event ?\n",
        "\n",
        "In this notebook, we are gonna show if a tweet concern a true or false event.\\\n",
        "We use Google trax.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sGQ_HIol4jd"
      },
      "source": [
        "## Packages\n",
        "Let's import packages we need\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezW6SRJ0mFTW",
        "outputId": "7e31aeba-52d7-48c3-bfce-30aa35184b69",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# coding : utf8\n",
        "!pip -q install trax\n",
        "!pip -q install Flask\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from unicodedata import normalize\n",
        "import trax\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import trax.layers as tl\n",
        "from trax.supervised import training "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 471kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 15.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 36.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 33.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 43.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 43.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 42.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: transformers 3.5.1 has requirement sentencepiece==0.1.91, but you'll have sentencepiece 0.1.94 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2WcxPEplYmH"
      },
      "source": [
        "## Dataset\n",
        "We use a dataset from a kaggle competition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtRekGL0kgn3",
        "outputId": "4479fdee-cc2e-4442-ac05-36d4a7b635c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Data importation\n",
        "data = pd.read_csv('train.csv', encoding = \"utf8\")\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id keyword  ...                                               text target\n",
            "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
            "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
            "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
            "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
            "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jua_xFsHm9Cs",
        "outputId": "7d97ec84-c83e-4223-e8b7-74a0d5f67bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Description of our Data.\n",
        "print(data.describe())\n",
        "print(\"Count values of target\")\n",
        "print(data.target.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 id      target\n",
            "count   7613.000000  7613.00000\n",
            "mean    5441.934848     0.42966\n",
            "std     3137.116090     0.49506\n",
            "min        1.000000     0.00000\n",
            "25%     2734.000000     0.00000\n",
            "50%     5408.000000     0.00000\n",
            "75%     8146.000000     1.00000\n",
            "max    10873.000000     1.00000\n",
            "Count values of target\n",
            "0    4342\n",
            "1    3271\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYbSPdOnOmb"
      },
      "source": [
        "# Create a list of tupe(sentence, target)\n",
        "from unicodedata import normalize\n",
        "\n",
        "\n",
        "def create_list_of_tuple(data, feature, target) : \n",
        "  \"\"\"\n",
        "  This function trasnform a data in list of tuple(feature, target)\n",
        "  input : data : a pandas dataframe\n",
        "  feature : feature : columns of data we need\n",
        "  target : target\n",
        "  output : t tuple(sentence, target)\n",
        "  \"\"\"\n",
        "  #if isinstance(data, pd.DataFrame) : \n",
        "  assert type(data) == pd.core.frame.DataFrame, 'Enter pandas dataframe '\n",
        "  assert type(feature) == str, 'Enter a list of feature'\n",
        "  assert type(target) == str, 'Enter a list of target'\n",
        "\n",
        "  t = []\n",
        "  for x,y in zip(data[feature].iloc, data[target].iloc): \n",
        "    t.append((x, y))\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3pt1h0wn40-",
        "outputId": "eb1be988-d933-45ee-9e27-022a1905032b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_l = create_list_of_tuple(data, \"text\", \"target\")\n",
        "print(\"Twio first line or data are : \", data_l[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Twio first line or data are :  [('Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', 1), ('Forest fire near La Ronge Sask. Canada', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqqPlC3rKfz"
      },
      "source": [
        "## Create a vocabulary\n",
        "Before creating of vocabulary, we will process our data. \n",
        "We will create some functions. \n",
        "process_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMo47R8ZrOhr",
        "outputId": "3f9d8b50-7dbc-4bc9-82e7-09bcd9fc16e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words(\"english\")\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "def process_tweet(tweet) : \n",
        "  stem = PorterStemmer()\n",
        "  assert type(tweet) == str, \"Input must be string\"\n",
        "  tweet = tweet.lower()\n",
        "\n",
        "  #tweet = re.sub(r'https?:\\/\\/\\w+', '<link> ', tweet)\n",
        "  tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '__link__', tweet)\n",
        "  # remove hashtags\n",
        "  # only removing the hash # sign from the word\n",
        "  tweet = re.sub(r'#|@', '', normalize(\"NFKC\", tweet))\n",
        "\n",
        "  tweet = re.sub(r'[\\[\\]|\\-!?:.,\\'()]', '', tweet)\n",
        "\n",
        "  tweet_clean = word_tokenize(tweet)\n",
        "  tweet_clean = [stem.stem(x) for x in tweet_clean if x not in sw]\n",
        "\n",
        "  return tweet_clean"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuYMpSB3iMER"
      },
      "source": [
        "Create a vocabulary from sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LemVsBU4t5RP",
        "outputId": "26acf976-3045-4e7a-f5aa-63b2fc37d712",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sent = \"320 [IR] ICEMOON [AFTERSHOCK] the | http://t.co/vAM5POdGyw | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/zEVakJaPcz lonely\"\n",
        "process_tweet(sent)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['320',\n",
              " 'ir',\n",
              " 'icemoon',\n",
              " 'aftershock',\n",
              " '__link__',\n",
              " 'djicemoon',\n",
              " 'dubstep',\n",
              " 'trapmus',\n",
              " 'dnb',\n",
              " 'edm',\n",
              " 'danc',\n",
              " 'ices\\x89û_',\n",
              " '__link__',\n",
              " 'lone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrvN8U_xyaym",
        "outputId": "bcaee187-ba34-46d2-bfcc-57978400f08b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import csv\n",
        "# Process tweets\n",
        "#from collections import defaultdict\n",
        "def get_dict(data_l) : \n",
        "  tokenized_l = []\n",
        "  word2ind = {\"__unk__\" : 1, \"__pad__\" : 0}\n",
        "  for tweet,target in data_l : \n",
        "    tokens = process_tweet(tweet)\n",
        "    tokenized_l.append((tweet, target))\n",
        "\n",
        "    # Create a word2ind dictionary\n",
        "    for word in tokens : \n",
        "      if word not in word2ind.keys() : \n",
        "        word2ind[word] = len(word2ind)\n",
        "\n",
        "  return word2ind\n",
        "\n",
        "# word2ind dictionary\n",
        "word2ind = get_dict(data_l)\n",
        "print(f\"Vocab size is : {len(word2ind)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size is  :15189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAJLq5KBLVOr"
      },
      "source": [
        "Define a tweet to tensor function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So0BFjzf-SKF"
      },
      "source": [
        "def tweet_to_tensor(tweet, unknown , vocab_dict) : \n",
        "  \"\"\"\n",
        "   This function trasnform a tweet to a tensor\n",
        "   Input : \n",
        "   tweet  : String\n",
        "   unknown : symbol of unknown word\n",
        "   vocab_dict :  a dictionary which map word to indice\n",
        "   output : \n",
        "   tensor  : list/array\n",
        "   \"\"\"\n",
        "  tokenized = process_tweet(tweet)\n",
        "\n",
        "  unk_id = vocab_dict[unknown]\n",
        "\n",
        "  tensor = []\n",
        "  for token in tokenized : \n",
        "    if token in vocab_dict.keys() : \n",
        "      tensor.append(vocab_dict[token])\n",
        "    else : \n",
        "      tensor.append(unk_id)\n",
        "  \n",
        "  return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdIeYuekLd4K"
      },
      "source": [
        "Define a data \n",
        "generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM7x_e6wXjJZ"
      },
      "source": [
        "import random\n",
        "\n",
        "def data_generator(data, batch_size, loop, vocab_dict, shuffle = False) : \n",
        "\n",
        "  #assert batch_size %2 == 0, \"batch size must be odd\"\n",
        "\n",
        "  # Set index to 0\n",
        "  index = 0\n",
        "\n",
        "  # Get len data\n",
        "  len_data = len(data)\n",
        "  \n",
        "  # Get and array with the data indexes\n",
        "  index_lines = list(range(len_data))\n",
        "  \n",
        "  # shuffle lines if shuffle is set to True\n",
        "  if shuffle:\n",
        "      random.shuffle(index_lines)\n",
        "\n",
        "  stop = False\n",
        "\n",
        "  # Loop indefintively\n",
        "  while not stop :\n",
        "\n",
        "    batch = [] # Create batch\n",
        "    target_l = [] # Create target\n",
        "\n",
        "    for i in range(batch_size) : \n",
        "\n",
        "      if index>=len_data : \n",
        "\n",
        "        # If loop is set to False, break once we reach the end of the dataset\n",
        "        if not loop : \n",
        "          stop = True\n",
        "          break\n",
        "\n",
        "        index = 0\n",
        "        # If user wants to keep re-using the data, reset the index\n",
        "        if shuffle : \n",
        "          random.shuffle(index_lines)\n",
        "\n",
        "\n",
        "      tweet = data[index_lines[index]][0]\n",
        "      target = data[index_lines[index]][1]\n",
        "\n",
        "      # convert the tweet into tensors of integers representing the processed words\n",
        "      tensor = tweet_to_tensor(tweet,'__unk__', vocab_dict)\n",
        "      #tensor = tokenize(tweet)\n",
        "\n",
        "      # append the tensor to the batch list\n",
        "      batch.append(tensor)\n",
        "      target_l.append(target)\n",
        "      \n",
        "    # Increment pos_index by one\n",
        "      index = index + 1\n",
        "\n",
        "    if stop:\n",
        "      break;\n",
        "\n",
        "  # Update the start index for positive data \n",
        "  # so that it's n_to_take positions after the current pos_index\n",
        "    index += batch_size\n",
        "\n",
        "    # Get the max tweet length (the length of the longest tweet) \n",
        "    # (you will pad all shorter tweets to have this length)\n",
        "    max_len = max([len(t) for t in batch]) \n",
        "    \n",
        "    \n",
        "    # Initialize the input_l, which will \n",
        "    # store the padded versions of the tensors\n",
        "    tensor_pad_l = []\n",
        "      # Pad shorter tweets with zeros\n",
        "    for tensor in batch:\n",
        "      n_pad = max_len - len(tensor)\n",
        "        \n",
        "      # concatenate the tensor and the list of padded zeros\n",
        "      tensor_pad = tensor + [0] * n_pad\n",
        "        \n",
        "      # append the padded tensor to the list of padded tensors\n",
        "      tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "      # convert the list of padded tensors to a numpy array\n",
        "      # and store this as the model inputs\n",
        "    inputs = np.array(tensor_pad_l)\n",
        "      \n",
        "      # Convert the target list into a numpy array\n",
        "    targets = np.array(target_l)\n",
        "\n",
        "      # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
        "    example_weights = np.ones_like(targets)\n",
        "\n",
        "    yield inputs, targets, example_weights\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6HhkxV9fHmY"
      },
      "source": [
        "stream = data_generator(data_l, 32, True, word2ind, shuffle = True)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGQTZBUwkenc",
        "outputId": "db25ea4d-3fec-4421-e901-45c95e511744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test data generator\n",
        "input,target, mask = next(stream)\n",
        "\n",
        "# Print shape of data generator outputs\n",
        "print(input.shape, target.shape, mask.shape)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 53) (32,) (32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPk_Vh5SnJAX",
        "outputId": "8f60d53e-3445-438d-a04b-509ca859fa2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Eval and train stream\n",
        "import random\n",
        "def train_test_set(data_l, eval_size = 0.05) : \n",
        "  \"\"\"\n",
        "  input : text : list of tuple of english sentence and its translation in french\n",
        "  output : train set and eval set\n",
        "  \"\"\"\n",
        "  random.shuffle(data_l)\n",
        "  length = len(data_l)\n",
        "  cutoff = int(length * (1-eval_size))\n",
        "  return data_l[:cutoff], data_l[cutoff:] \n",
        "\n",
        "# Get train and test set\n",
        "train_set, eval_set = train_test_set(data_l, 0.15)\n",
        "\n",
        "# print length of train and test set\n",
        "print(len(train_set), len(eval_set))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6471 1142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpdvMOdwooSg"
      },
      "source": [
        "# Create the training data generator\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_set, batch_size, True, word2ind, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(eval_set, batch_size, True, word2ind, shuffle)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIboV6zRX_eZ"
      },
      "source": [
        "def classifier(vocab_size=len(word2ind), embedding_dim=2048, output_dim=2, mode='train'):\n",
        "        \n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding(\n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim)  # Embedding dimension\n",
        "    \n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis = 1, keepdims= False)\n",
        "\n",
        "    lstm = tl.LSTM(embedding_dim)\n",
        "    \n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    \n",
        "    # Create the log softmax layer (no parameters needed)\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "    \n",
        "    # Use tl.Serial to combine all layers\n",
        "    # and create the classifier\n",
        "    # of type trax.layers.combinators.Serial\n",
        "    model = tl.Serial(\n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer, # dense output layer \n",
        "      log_softmax_layer # log softmax layer\n",
        "    )\n",
        "### END CODE HERE ###     \n",
        "    \n",
        "    # return the model of type\n",
        "    return model"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNPyKcvSm3Rh"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create a train and test task for training\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.0001),\n",
        "    n_steps_per_checkpoint=10,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWb4S9GbpX-a",
        "outputId": "ac056dcc-1b0f-43c9-fd3f-036ed9e766d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "output_dir = '/model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)\n",
        "\n",
        "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: train_model\n",
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_tasks = eval_task, # The evaluation task\n",
        "                                output_dir = output_dir) # The output directory\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "### END CODE HERE ###\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ihBmQL4PiP",
        "outputId": "57094919-af97-4263-801b-cda3653bccc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = classifier()\n",
        "print(model)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Embedding_15189_2048\n",
            "  Mean\n",
            "  Dense_2\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWw8Ek8RpdYz",
        "outputId": "c436f8ec-348f-46e4-8720-70f0b75897f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "training_loop = train_model(model, train_task, eval_task, 100, '.')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step    210: Ran 10 train steps in 20.56 secs\n",
            "Step    210: train CrossEntropyLoss |  0.64818811\n",
            "Step    210: eval  CrossEntropyLoss |  0.66104668\n",
            "Step    210: eval          Accuracy |  0.57812500\n",
            "\n",
            "Step    220: Ran 10 train steps in 18.29 secs\n",
            "Step    220: train CrossEntropyLoss |  0.65991086\n",
            "Step    220: eval  CrossEntropyLoss |  0.65271246\n",
            "Step    220: eval          Accuracy |  0.57812500\n",
            "\n",
            "Step    230: Ran 10 train steps in 18.22 secs\n",
            "Step    230: train CrossEntropyLoss |  0.65608096\n",
            "Step    230: eval  CrossEntropyLoss |  0.64291835\n",
            "Step    230: eval          Accuracy |  0.67187500\n",
            "\n",
            "Step    240: Ran 10 train steps in 17.94 secs\n",
            "Step    240: train CrossEntropyLoss |  0.66375715\n",
            "Step    240: eval  CrossEntropyLoss |  0.65520656\n",
            "Step    240: eval          Accuracy |  0.59375000\n",
            "\n",
            "Step    250: Ran 10 train steps in 18.94 secs\n",
            "Step    250: train CrossEntropyLoss |  0.65612048\n",
            "Step    250: eval  CrossEntropyLoss |  0.68603837\n",
            "Step    250: eval          Accuracy |  0.54687500\n",
            "\n",
            "Step    260: Ran 10 train steps in 17.82 secs\n",
            "Step    260: train CrossEntropyLoss |  0.65083182\n",
            "Step    260: eval  CrossEntropyLoss |  0.64537436\n",
            "Step    260: eval          Accuracy |  0.59375000\n",
            "\n",
            "Step    270: Ran 10 train steps in 17.85 secs\n",
            "Step    270: train CrossEntropyLoss |  0.64458227\n",
            "Step    270: eval  CrossEntropyLoss |  0.66286874\n",
            "Step    270: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step    280: Ran 10 train steps in 18.06 secs\n",
            "Step    280: train CrossEntropyLoss |  0.65813118\n",
            "Step    280: eval  CrossEntropyLoss |  0.63806498\n",
            "Step    280: eval          Accuracy |  0.64062500\n",
            "\n",
            "Step    290: Ran 10 train steps in 17.75 secs\n",
            "Step    290: train CrossEntropyLoss |  0.65916842\n",
            "Step    290: eval  CrossEntropyLoss |  0.64075601\n",
            "Step    290: eval          Accuracy |  0.62500000\n",
            "\n",
            "Step    300: Ran 10 train steps in 17.28 secs\n",
            "Step    300: train CrossEntropyLoss |  0.63496602\n",
            "Step    300: eval  CrossEntropyLoss |  0.65138537\n",
            "Step    300: eval          Accuracy |  0.64062500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR_5UZitu0eh",
        "outputId": "131356d6-5894-48c5-b630-8e94671c20e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a generator object\n",
        "tmp_train_generator = train_generator(16)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
        "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
        "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
        "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")\n",
        "\n",
        "\n",
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print()\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
            "The shape of the tweet tensors is (16, 26) (num of examples, length of tweet tensors)\n",
            "The shape of the labels is (16,), which is the batch size.\n",
            "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n",
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "\n",
            "View the prediction array\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.02711511, -3.6211896 ],\n",
              "             [-4.0115175 , -0.01827192],\n",
              "             [-0.02149475, -3.8506737 ],\n",
              "             [-0.20396167, -1.6900711 ],\n",
              "             [-0.41540757, -1.0790191 ],\n",
              "             [-0.10753   , -2.2832696 ],\n",
              "             [-0.50392365, -0.92673385],\n",
              "             [-0.8582263 , -0.55149704],\n",
              "             [-1.5595372 , -0.2360177 ],\n",
              "             [-0.08233225, -2.5378766 ],\n",
              "             [-0.01742125, -4.0587654 ],\n",
              "             [-0.03321397, -3.4213448 ],\n",
              "             [-3.3148766 , -0.0370152 ],\n",
              "             [-0.7447773 , -0.6440523 ],\n",
              "             [-0.11672461, -2.2057328 ],\n",
              "             [-1.7174097 , -0.19787866]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLAUME5RyFK5",
        "outputId": "6bf5976e-1b7d-40c0-97c0-24ab79b1b170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# turn probabilites into category predictions\n",
        "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
        "for i, p in enumerate(tmp_is_positive):\n",
        "    print(f\"Neg log prob {np.exp(tmp_pred[i,0]):.4f}\\tPos log prob {np.exp(tmp_pred[i,1]):.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg log prob 0.9732\tPos log prob 0.0268\t is positive? False\t actual 0\n",
            "Neg log prob 0.0181\tPos log prob 0.9819\t is positive? True\t actual 1\n",
            "Neg log prob 0.9787\tPos log prob 0.0213\t is positive? False\t actual 0\n",
            "Neg log prob 0.8155\tPos log prob 0.1845\t is positive? False\t actual 1\n",
            "Neg log prob 0.6601\tPos log prob 0.3399\t is positive? False\t actual 1\n",
            "Neg log prob 0.8980\tPos log prob 0.1020\t is positive? False\t actual 0\n",
            "Neg log prob 0.6042\tPos log prob 0.3958\t is positive? False\t actual 0\n",
            "Neg log prob 0.4239\tPos log prob 0.5761\t is positive? True\t actual 1\n",
            "Neg log prob 0.2102\tPos log prob 0.7898\t is positive? True\t actual 1\n",
            "Neg log prob 0.9210\tPos log prob 0.0790\t is positive? False\t actual 0\n",
            "Neg log prob 0.9827\tPos log prob 0.0173\t is positive? False\t actual 1\n",
            "Neg log prob 0.9673\tPos log prob 0.0327\t is positive? False\t actual 0\n",
            "Neg log prob 0.0363\tPos log prob 0.9637\t is positive? True\t actual 1\n",
            "Neg log prob 0.4748\tPos log prob 0.5252\t is positive? True\t actual 0\n",
            "Neg log prob 0.8898\tPos log prob 0.1102\t is positive? False\t actual 0\n",
            "Neg log prob 0.1795\tPos log prob 0.8205\t is positive? True\t actual 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_SHTXAYBSxe"
      },
      "source": [
        "predict = classifier()\n",
        "# instantiate the model we built in eval mode\n",
        "predict = classifier(mode = \"eval\")\n",
        " \n",
        "# initialize weights from a pre-trained model\n",
        "predict.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
        "predict = tl.Accelerate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ9NMwLAMFaI"
      },
      "source": [
        "modele = classifier()\n",
        "modele.init_from_file('model.pkl.gz')\n",
        "\n",
        "def prediction(sentence, model = model, vocab_dict = word2ind) :\n",
        "\n",
        "  input = np.array(tweet_to_tensor(sentence, '__unk__', vocab_dict))[None, :]\n",
        "\n",
        "  preds = model(input)\n",
        "  classification = np.argmax(preds)\n",
        "  prob = np.exp(preds[0, classification])\n",
        "\n",
        "  return abs(1-classification), prob "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moy9jecZKDLB"
      },
      "source": [
        "Prediction for test few sentence of test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTaXZz0mM36v",
        "outputId": "6e5d7281-7072-46d1-d65a-a2353b0fbf8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "sentences = list(np.squeeze(test.text))\n",
        "random.shuffle(sentences)\n",
        "i = 0\n",
        "for sentence in sentences : \n",
        "  i += 1\n",
        "  print(sentence, prediction(sentence))\n",
        "  if i > 100 : \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wildfire near Columbia River town is 50 percent contained - http://t.co/yTPiPXpqr9 http://t.co/HCUQ6jpBtL (0, 0.9975071)\n",
            ".@MartinMJ22 @YouGov Which '#Tory landslide' ... you can't POSSIBLY mean the wafer-thin majority of #GE2015?!! (1, 0.7468)\n",
            "Amazing how smug this guy is about the use of mass murder as a global power play. https://t.co/AlTTBJy8B9 (1, 0.7391276)\n",
            "IF SUICIDE BOMBING WASTHE SMARTEST THING2 DO FOR ALLAH/GODJESUS/THE HOLY PROPHET MUHAMMAD COULD HAVE KILLEDSOMEBODY? http://t.co/tGfWuVVHxj (0, 0.9681884)\n",
            "Waige driving like a Badass #ScorpionPilot #Cyclone @ScorpionCBS (0, 0.79837376)\n",
            "@Riot_Sweet @jfwong @MtgoDoc @JoshLeeKwai wait.. what dates?? ill be at MSG and PAX =( (1, 0.87318)\n",
            "I BOMBED THE FAKE BOLT CHAT AND EVERYONE THOUGHT BOLT ENDED LMFAO (1, 0.5799052)\n",
            "If our love is tragedy why are you my remedyyyy (1, 0.9979462)\n",
            "I got more rhymes than the Bible's got psalms\n",
            "And just like the Prodigal Son I've returned\n",
            "Anyone stepping to me you'll get burned\n",
            "?? (1, 0.96373075)\n",
            "Investigators shift focus to cause of fatal Waimate fire http://t.co/ej0mewOupS (0, 0.9998965)\n",
            "@DebtAssassin They forget there is the threat of bioterrorism. (1, 0.94787234)\n",
            "did leona lewis ever see that video of dinah singing bleeding love? (1, 0.9941575)\n",
            "Listen to Apollo Brown - Detonate (feat. M.O.P.) by Mello Music Group #np on #SoundCloud https://t.co/C0Fex1XAlG (1, 0.9934962)\n",
            "this thunder though....... (0, 0.9965413)\n",
            "The #Future of Insurance and Driverless #Cars: Can the #Insurance Industry Survive #Driverless Cars?\n",
            "http://t.co/zEARtXBOFt (1, 0.6212223)\n",
            "Before &amp; After #underconstruction #demolished #Melbourne #Residential #commercial #builder #luxuryhomes #townhouses ?? http://t.co/6iD7CZvMEd (0, 0.62728876)\n",
            "@TokyoDotCom I didn't either till it blew up my mentions lol (1, 0.9817986)\n",
            "Twin Storms Blow Through Calgary * 75 http://t.co/sIAKlSbdiP http://t.co/fm44ZS93OA (0, 0.9947469)\n",
            "Love wounds (1, 0.99224657)\n",
            "You still acting like you were the one wounded. Didn't you do the stabbing?... (1, 0.9147576)\n",
            "Which version of All Hail Shadow you do you like more Magna-Fi's or Crush 40's? ÛÓ Crush 40s. http://t.co/oGs6d0p3RB (1, 0.9246933)\n",
            "Stop the Annihilation of the Salt River Wild Horses!!! https://t.co/546utTXzAm via @Change (0, 0.75707614)\n",
            "Whoa! This new legend will be fun! http://t.co/hu5CmoupUM (1, 0.99810493)\n",
            "Stuart Broad Takes Eight Before Joe Root Runs Riot Against Aussies: Stuart Broad took career-best figures of 8... http://t.co/2UmwMG7lvN (1, 0.63458)\n",
            "Gonna throw a huge party and play nothing but Sandstorm. (1, 0.6523431)\n",
            "China's Stock Market Crash: Are There Gems In The Rubble?: ChinaÛªs stock market crash this summer has sparked ... http://t.co/vZDqQ9yPN1 (1, 0.98590034)\n",
            "Protesters mark year since fatal police shooting in Wal-Mart (from @AP) http://t.co/3KbeJGmj0d (0, 0.9998584)\n",
            "@CoolBreezeT train derailed at Smithsonian...no passenger train...then my red line train became disabled between FH &amp; Bethesda (0, 0.9732013)\n",
            "Emergency prompted due to chemical spill at a factory near RedruthåÊUK https://t.co/oiCtAcbwSR http://t.co/46lSIanjMC (0, 0.9967478)\n",
            "Literally for lunch i had cheese curds and for dinner i had a blizzard pls pray for my health (0, 0.5200904)\n",
            "@OrianaArzola does he not have any manners or something? Jfc. You have all the rights to be mad! But hey try not to let this ruin your day (1, 0.9980345)\n",
            "@MikeParrActor devastated your no longer in emmerdale best character with so much more to give #superbactor your going to be missed (1, 0.9413908)\n",
            "Historical Bigfoot Landmark Soon To Be Demolished: The Bluff Creek Resort Store pictured above in 1976 was t... http://t.co/TW0SEv1N6C (1, 0.5732735)\n",
            "@quinhii I'm the one who started it so I feel like I have so much responsibility bUT NO ONE DOES ANYTHING OTL (a 'national' disaster lol) (1, 0.9497944)\n",
            "#Bestnaijamade: 16yr old PKK suicide bomber who detonated bomb in ... http://t.co/KSAwlYuX02 bestnaijamade bestnaijamade bestnaijamade beÛ_ (0, 0.99972683)\n",
            "Related News: The Bureaucrats Who Singled Out Hiroshima for Destruction - Global - The Atlantic |  http://t.co/Tnex4HUsnp (0, 0.5926862)\n",
            "I have the biggest girl crush on oomf ???? (1, 0.99954903)\n",
            "We're shaking...It's an earthquake (0, 0.99999714)\n",
            "Trim error led to fatal Greek F-16 crash: ?A NATO safety investigation board report has determined that the ma... http://t.co/YhSahLKQo4 (0, 0.594326)\n",
            "@StephGHinojosa hey my mom said to tell your dad to call her ASAP. it's an emergency. (1, 0.7949507)\n",
            "Suicide bomber kills 15 in Saudi security site mosque - Reuters http://t.co/KCObrZBVDs http://t.co/y62HSFVIAQ (0, 0.9999876)\n",
            "Tuesday Bolts ÛÒ 8.4.15 http://t.co/mkMSV54eV2 #Thunder #NBA (0, 0.9329607)\n",
            "@itsTiimothy bhill bruh you can obliterate beez (1, 0.9970684)\n",
            "Do people not understand they cant bleeding tweet everyone all the timedoes me head in 'stop ignoring me' they are hardly ignoring you+ (1, 0.9088227)\n",
            "@AParra210 So you are stereotyping the people. Are you a mass murderer like today's shooter? (0, 0.82726437)\n",
            "RT @judithabarrow author CHANGING PATTERNS  #amazon http://t.co/0oCrOtMkHM SILENT TRAUMA http://t.co/2ZGrVnMDW9 #amazon (0, 0.5457251)\n",
            "@christinaperri #askceeps have you seen @colinodonoghue1 in his new movie trailer for The Dust Storm? (1, 0.8647985)\n",
            "New Boonie Hat USMC Airsoft Paintball Military Cap Woodland Camo CP030 http://t.co/EGqE35I5pI http://t.co/uaBRYnHkn7 (1, 0.941099)\n",
            "Tonight It's Going To Be Mayhem @ #4PlayThursdays. Everybody Free w/ Text. 1716 I ST NW (18+) http://t.co/omYWCLpGEf (1, 0.9776038)\n",
            "Roquey and obliterated toy mollusk https://t.co/1qnRBXFr2v (1, 0.5700927)\n",
            "Japan marks 70th anniversary of Hiroshima atomic bombing: Bells tolled in Hiroshima on Thursday as Japan marke... http://t.co/IqAIRPdIhg (0, 0.99999475)\n",
            "13 reasons why we love women in the military   - lulgzimbestpicts http://t.co/tRgPWOuj6X http://t.co/GEMNUZoCeb (1, 0.90640736)\n",
            "CommoditiesåÊAre Crashing Like It's 2008 All Over Again http://t.co/EM1cN7alGk (1, 0.903203)\n",
            "omg it is I can go in dungeons with Landslide Krag now &lt;3 (1, 0.9440867)\n",
            "A sinkhole grows ... in Brooklyn? http://t.co/UiqKDdVwKz http://t.co/aa8FwtOHyJ (0, 0.9979684)\n",
            "#BigData Deluge is out! http://t.co/khatZh7agZ (1, 0.9979698)\n",
            "@myralynn24 Becuase it can cause property damage and cause wild fires (0, 0.99988747)\n",
            "...@jeremycorbyn must be willing to fight and 2 call a spade a spade. Other wise very savvy piece by @OwenJones84 \n",
            "http://t.co/fabsyxQlQI (0, 0.74042225)\n",
            "@preeti_chopra2 @chinmaykrvd many predicted landslide for nitish nd told bjp will loose bihar nd won't even get upper castes votes (1, 0.58406574)\n",
            "S3XLEAK!!!\n",
            "Ph0tos of 19yrs old Ash@wo lady in Festac town from Delta exp0sed on BBM 5 leaked pictures... http://t.co/lUm4l65alz (0, 0.738061)\n",
            "Your brain is particularly vulnerable to trauma at two distinct ages http://t.co/RAv8iMVvZB via @qz (1, 0.82962334)\n",
            "@BuzzFeed Stannis is not evil in the books GOT is a catastrophe just saying (1, 0.9912866)\n",
            "#BestSeller The trouble in one of Buffett's favorite sectors http://t.co/4wa9cMs3Cz (1, 0.67130417)\n",
            "Israeli police unable to solve the case of Duma arson attack http://t.co/WtZgXzaf7Z (0, 0.9988977)\n",
            "@TheFieryGrave yeah true because if he wouldn't have got exposed then he probably wouldn't have went to mlg and blew up there (1, 0.9891382)\n",
            "Swansea ?plot hijack transfer move for Southampton target Virgil van Dijk? http://t.co/DNZjWwTAkI (1, 0.84214765)\n",
            "This week we received the sad news of our friends daughters cancer prognosis. This is the second upheaval in this... http://t.co/tpOptWMWFR (1, 0.9468673)\n",
            "Finnish Nuclear Plant to Move Ahead After Financing Secured http://t.co/S9Jhcf3lD7 @JukkaOksaharju @ollirehn @juhasipila (0, 0.7059823)\n",
            "'When you attack PP you attack women's health &amp; when you attack women's health you attack America's health.' http://t.co/HXdG24MCQg (0, 0.95765734)\n",
            "@Chelsea_Rosette ah bless ya. Not too bad. My life's been a bit of a whirlwind the last two weeks! Got a cold too that won't shift ?? #sexy (1, 0.9873791)\n",
            "Short story about indifference oppression hatred anger hope inspiration revolution retaliation and obliteration.  \n",
            "\n",
            "'Fuck Them' (1, 0.8960091)\n",
            "Police Officer Wounded Suspect Dead After Exchanging Shots: Richmond police officer wounded suspect killed a... http://t.co/YIKQRqczVZ (0, 0.99999666)\n",
            "Top story: @ViralSpell: 'Couple spend wedding day feeding 4000 Syrian refugeesÛ_ http://t.co/QAYayzh0fL see more http://t.co/oTqbHd2akn (1, 0.9533104)\n",
            "Police Officer Wounded Suspect Dead After Exchanging Shots http://t.co/e4wvXQItV4 (0, 0.9999957)\n",
            "ÛÏ@beachboyIrh: AT GMA THE SECURITY GUY SAID 'THIS IS A FAMILY SHOW' AND EVERYONE SCREAMED 'OOORRRR IS IT'Û I love this fandom (1, 0.98578495)\n",
            "The tap in the bath exploded and now it looks like someone shit in the bath. I love how great our plumbing is!???? (1, 0.9701507)\n",
            "LIVE on #Periscope: Wild Wing Epicentre https://t.co/U2fUK072F9 (0, 0.9920902)\n",
            "New RAN report from the frontlines of human rights abuses and forest destruction for fashion.: http://t.co/5xyE2Rkuri (1, 0.66035926)\n",
            "of all days when I look like death I have to see all of mo city in chick fil a today ?? (1, 0.7313463)\n",
            "@KrisZellner But Triplemania has the promise of Villano bleeding and Del Rio whooping that Trick Myztiziziziziz. (1, 0.970524)\n",
            "PSA: IÛªm splitting my personalities.\n",
            "\n",
            "?? techies follow @ablaze_co\n",
            "?? Burners follow @ablaze (1, 0.78206116)\n",
            "Militant 'Overpowered' By Hostages After Attack http://t.co/xWnp8a1JOn (0, 0.99952877)\n",
            "@froilan_canin https://t.co/caCXADLEb8 AFTERNOON DELIGHT Ha! Bounce filmed in between Calgary's Uber Hail Storms #yyc http://t.co/6zgkDpMMSK (0, 0.99362886)\n",
            "@trap_vodka my new fave vine (1, 0.99998283)\n",
            "Politicians are using false allegations to attack #PlannedParenthood &amp; harm women. We aren't fooled we #StandwithPP http://t.co/JhseGQLbYq (0, 0.67380506)\n",
            "@puzzledtriangle please inform me of that date because that song very accurately reflects the panic of the week before (1, 0.9680224)\n",
            "Baseball: Eyewitness Accounts: August 6 2015 by Jeff Moore Wilson Karaman and Brendan Gawlowski: Eyes on Mi http://t.co/H4ZOZqLpE7 #sport (1, 0.78987885)\n",
            "...//..// whao.. 12000 Nigerian refugees repatriated from Cameroon http://t.co/aVwE1LBvhn (0, 0.9990454)\n",
            "Picture of cute sleeping puppy photo bombed by the demon in the back. https://t.co/LprYzSTK4u (0, 0.5245344)\n",
            "Bin Laden family plane crashed after 'avoiding microlight and landing too far down runway': Three members of t... http://t.co/ZlBOeMXo2T (0, 0.99050575)\n",
            "Apocalypse no! Why artists should not go into the #Fukushima exclusion zone \n",
            "http://t.co/3zqL0qbLUw\n",
            "#nuclear #ura (0, 0.95841867)\n",
            "100  1' MIX NEW FLAT DOUBLE SIDED LINERLESS BOTTLE CAPS YOU CHOOSE MIX FLATTENED - Full reÛ_ http://t.co/61fALvOCuK http://t.co/1MuTpFcgDL (1, 0.9936959)\n",
            "Suicide Bomber Kills 13 At Saudi Mosque http://t.co/VHQeD0gO0n (0, 0.9999981)\n",
            "Las Vegas in top 5 cities for red-light running fatalities - News3LV http://t.co/rEt82MdQnG (0, 0.755985)\n",
            "U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/FLcQQeZnVW via @Change (1, 0.50832915)\n",
            "#CANCER : everything seems catastrophic you feel you can not be worse. Hit rock bottom. You recover. You start again. After a while got it (1, 0.95544016)\n",
            "PODCAST: Oil spill anniversary http://t.co/wVdAVXTDaq (0, 0.99987984)\n",
            "Call to car vs fed ex truck head on collision on I-40 mile marker 118. Prayers for the families as we get to the scene! (0, 0.7625728)\n",
            "Road Hazard @ E CONFEDERATE AVE SE / MORELAND AVE SE http://t.co/tym6tYmh4M (0, 0.7883939)\n",
            "RT MME_AUSTIN: Why #Marijuana Is Critical For Research in Treating #PTSD\n",
            "\n",
            "http://t.co/T6fuAhFp7p \n",
            "#hempoil #cannÛ_ http://t.co/RhE7dXM7Ey (0, 0.7493673)\n",
            "Pelling hotels: no strings concealment from straight a rejuvenati???ng evacuation day: pqhaxp (0, 0.882568)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}